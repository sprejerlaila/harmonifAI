{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['including_rests_clean-Copy1.ipynb', '.DS_Store', 'get_harmony.ipynb', 'encoder_model.json', 'get_music.ipynb', 'best_model_farm.h5', 'encoder_model.h5', 'target_token_index.pkl', 'all_4.py', 'dict.pkl', 'processed_songs.pkl', '.ipynb_checkpoints', 'decoder_model.h5', 'run_model.ipynb', 'reverse_target_char_index.pkl', 'get_data-Copy1.ipynb', 'decoder_model.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.options.display.max_rows = 999\n",
    "# Listing current data on our folder.\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "from music21 import converter, corpus, instrument, midi, note, chord, pitch, roman\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Building a english to french translator\n",
    "\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "#https://github.com/devm2024/nmt_keras/blob/master/base.ipynb\n",
    "#https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\n",
    "\n",
    "import h5py\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open(\"encoder_model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "encoder_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "encoder_model.load_weights(\"encoder_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "json_file = open(\"decoder_model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "decoder_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "decoder_model.load_weights(\"decoder_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "with open('target_token_index.pkl', 'rb') as picklefile:\n",
    "    target_token_index = pickle.load(picklefile)\n",
    "\n",
    "with open('reverse_target_char_index.pkl', 'rb') as picklefile:\n",
    "    reverse_target_char_index = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_max = 16\n",
    "\n",
    "zeros = [0]*23 # 23 = number of unique notes + rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict.pkl', 'rb') as picklefile:\n",
    "    d = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing music functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_midi(midi_path, remove_drums):\n",
    "    '''\n",
    "    There is an one-line method to read MIDIs\n",
    "    but to remove the drums we need to manipulate some\n",
    "    low level MIDI events.\n",
    "    '''\n",
    "    \n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(midi_path)\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "    if (remove_drums):\n",
    "        for i in range(len(mf.tracks)):\n",
    "            mf.tracks[i].events = [ev for ev in mf.tracks[i].events if ev.channel != 10]\n",
    "            # By convention track 10 is reserved for percussion\n",
    "    melody_track = \"\"\n",
    "    \n",
    "    '''\n",
    "    It is hard to identify the melody track. I'll do my best by looking for keywords, \n",
    "    but even if I'm confusing the melody with the base line I'm assuming it is still \n",
    "    useful to capture the relationship between that and the harmony of the song.\n",
    "    '''\n",
    "    \n",
    "    for track in range(len(mf.tracks)):\n",
    "            if 'lead' in str(mf.tracks[track].events[1].data).lower() or 'voice' in str(mf.tracks[track].events[1].data).lower() or 'melody' in str(mf.tracks[track].events[1].data).lower() or 'karaoke' in str(mf.tracks[track].events[1].data).lower():\n",
    "                melody_track = track - 1\n",
    "                break\n",
    "    if melody_track == \"\":\n",
    "        melody_track = 0\n",
    "        # By convention the first track is the melody\n",
    "    return (midi.translate.midiFileToStream(mf),melody_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_unique = ['i','bi','#i','ii','bii','#ii','iii','biii','#iii',\n",
    "  'iv','biv','#iv','v','bv','#v','vi','bvi','#vi',\n",
    "  'vii','bvii','#vii','rest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melo_encoded(df,melody_track):\n",
    "    ''' This function returns the encoded melody'''\n",
    "    music_key = df.analyze('key')\n",
    "    time = []\n",
    "    melody_roman = []\n",
    "    duration = []\n",
    "    for nt in (df.parts[melody_track].notesAndRests):\n",
    "        note_array = np.zeros(len(notes_unique)+1) # add one place for the duration of the note\n",
    "        if isinstance(nt, note.Note):     \n",
    "            time.append(float(nt.offset))\n",
    "            roman_chord = chord.Chord(nt.pitch.name + \" \" + \"C\") # for some reason Music21 doesn't allow to \n",
    "            roman_chord.remove('C') # convert augmented notes into a chord, so I add and remove 'C' and it works\n",
    "            roman_chord_numeral = roman.romanNumeralFromChord(roman_chord,music_key).figure\n",
    "            note_array[notes_unique.index(roman_chord_numeral)] = 1\n",
    "            note_array[22] = nt.duration.quarterLength\n",
    "            melody_roman.append(note_array)\n",
    "        elif isinstance(nt, note.Rest):\n",
    "            time.append(float(nt.offset))\n",
    "            note_array[21] = 1\n",
    "            note_array[22] = nt.duration.quarterLength\n",
    "            melody_roman.append(note_array)\n",
    "    \n",
    "            \n",
    "    melody = pd.DataFrame({'offset':time,'input':melody_roman})\n",
    "    melody['group'] = melody['offset'].apply(lambda x: np.floor(x/float(4))) # split sequences in groups of 4 offsets\n",
    "    return melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chords(df):\n",
    "    '''Using chordify function extract the harmony of the song'''\n",
    "    music_key = df.analyze('key')\n",
    "    df_chordify = df.chordify()\n",
    "\n",
    "    time = []\n",
    "    chordify = []\n",
    "    \n",
    "    for thisChord in df_chordify.recurse().getElementsByClass('Chord'):\n",
    "        time.append(float(thisChord.offset))\n",
    "        chordify.append(simplify_roman_name(thisChord, music_key))\n",
    "\n",
    "    chordify_df = pd.DataFrame({'offset':time,'target':chordify})\n",
    "    chordify_df['group'] = chordify_df['offset'].apply(lambda x: np.floor(x/float(4)))\n",
    "    return chordify_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_roman_name(thisChord, music_key):\n",
    "    global d\n",
    "    if d[(str(thisChord), str(music_key))]:\n",
    "        return d[(str(thisChord), str(music_key))]\n",
    "    roman_numeral = roman.romanNumeralFromChord(thisChord, music_key)\n",
    "    '''Thanks @wfaria for this code! https://www.kaggle.com/wfaria/midi-music-data-extraction-using-music21/notebook\n",
    "    in this method we try to simplify names, even if it ends in\n",
    "    a different chord to reduce the chord vocabulary and reduce the number of classes for the decoder model.'''\n",
    "    \n",
    "    \n",
    "    ret = roman_numeral.romanNumeral\n",
    "    inversion_name = None\n",
    "    inversion = roman_numeral.inversion()\n",
    "    \n",
    "    # Checking valid inversions.\n",
    "    if ((roman_numeral.isTriad() and inversion < 3) or\n",
    "            (inversion < 4 and\n",
    "                 (roman_numeral.seventh is not None or roman_numeral.isSeventh()))):\n",
    "        inversion_name = roman_numeral.inversionName()\n",
    "        \n",
    "    if (inversion_name is not None):\n",
    "        ret = ret + str(inversion_name)\n",
    "        \n",
    "    elif (roman_numeral.isDominantSeventh()): ret = ret + \"M7\"\n",
    "    elif (roman_numeral.isDiminishedSeventh()): ret = ret + \"o7\"\n",
    "    d[(str(thisChord), str(music_key))] = ret\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_measure_encoded(df):\n",
    "    ''' We group melodies and harmonies by 4 offsets'''\n",
    "    \n",
    "    df = df.fillna(\"null\")\n",
    "    grouped = df.groupby('group_x')\n",
    "    input_texts = np.array([np.array(list(grouped.get_group(x)['input'])) for x in grouped.groups])\n",
    "    target_texts = [list(grouped.get_group(x)['target']) for x in grouped.groups]\n",
    "    group = [x for x in grouped.groups]\n",
    "    target_texts = [' '.join(x) for x in target_texts]\n",
    "    return pd.DataFrame({'group':group,'input':input_texts,'target':target_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melo_chord(df,melody_track):\n",
    "    ''' Returns the final dataframe with the melody and the harmony'''\n",
    "    melo = get_melo_encoded(df,melody_track)\n",
    "    chords = get_chords(df)    \n",
    "    melo_chords = melo.merge(chords,on='offset',how='left')\n",
    "    grouped  = group_measure_encoded(melo_chords)\n",
    "\n",
    "    final = (grouped\n",
    "             .sort_values(by='group')\n",
    "             .rename(columns={'input':'melody','target':'harmony'})\n",
    "             .fillna('rest')) \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_note(array):\n",
    "    ''' Returns the note in roman notation '''\n",
    "    try:\n",
    "        note_index, = np.where(array[:-1] == 1)\n",
    "        return (notes_unique[note_index[0]],array[-1])\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    ''' Returns the harmony in roman notation '''\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_diatonic(rom_chord, music_key):\n",
    "    ''' Check if a given chord is diatonic '''\n",
    "    if 'major' in music_key.name:\n",
    "        diatonic_chords = ['i','I','ii','iii','III','IV','V','vi','vii']\n",
    "    else:\n",
    "        diatonic_chords = ['i','ii','III','iv','v','V','VI','VII']\n",
    "    if re.sub(r\"[1-9]\", \"\", rom_chord) in diatonic_chords:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_to_int = {'I':0,'II':1,'III':2,'IV':3,'V':4,'VI':5,'VII':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diatonic_chord(chord, music_key):\n",
    "    ''' If the chord is not diatonic, convert it to diatonic '''\n",
    "    if 'major' in music_key.name:\n",
    "        diatonic_chords = ['I7','ii7','iii','IV7','V7','vi','vii5b7']\n",
    "    else:\n",
    "        diatonic_chords = ['i7','ii7','III','iv7','V7','VI','VII']\n",
    "    if not is_diatonic(chord,music_key):\n",
    "        chord = re.sub(r\"[1-9-b]\", \"\", chord).upper()\n",
    "        chord = diatonic_chords[roman_to_int[chord]]\n",
    "    return chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeral_chords(dec_input,decoded_sentence,k):\n",
    "    ''' Returns chords in the key of the song.\n",
    "    Initially, each chord is gonna have the duration of the note of the melody'''\n",
    "    notes = [x[0] for x in dec_input if x!=\"\"]\n",
    "    durations = [x[1] for x in dec_input if x!=\"\"]\n",
    "\n",
    "    decoded_chords = []\n",
    "    rom_chords = decoded_sentence.split(\" \")[1:]\n",
    "\n",
    "    for i in range(len(durations)):\n",
    "        try:\n",
    "            if notes[i] == 'rest':\n",
    "                tonic_chord = note.Rest()\n",
    "                tonic_chord.duration.quarterLength = durations[i]\n",
    "            else:\n",
    "                diatonic_roman = get_diatonic_chord(rom_chords[i],k)\n",
    "                tonic_chord = chord.Chord(roman.RomanNumeral(diatonic_roman,k).pitches)\n",
    "                tonic_chord.duration.quarterLength = durations[i]\n",
    "            decoded_chords.append(tonic_chord)\n",
    "        except: # If there are less chords that notes of the melody\n",
    "            tonic_chord = note.Rest()\n",
    "            tonic_chord.duration.quarterLength = durations[i]\n",
    "            decoded_chords.append(tonic_chord)\n",
    "            pass\n",
    "    return decoded_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_chords(decoded_chords):\n",
    "    ''' Reduce total number of chords to only two.\n",
    "    The position of the chords is gonna be random'''\n",
    "    start_chord =random.randint(0,len(decoded_chords)-2)\n",
    "\n",
    "    end_chord = random.randint(start_chord+1,len(decoded_chords)-1)\n",
    "\n",
    "    chord1 = decoded_chords[start_chord]\n",
    "    for ch in decoded_chords[start_chord:end_chord]:\n",
    "        if chord1 != ch:\n",
    "            #chord1 = get_diatonic_chord(chord1)\n",
    "            chord1.duration.quarterLength += ch.duration.quarterLength\n",
    "            if isinstance(chord1, chord.Chord):\n",
    "                chord1.closedPosition(forceOctave=3, inPlace=True)\n",
    "\n",
    "    chord2 = decoded_chords[end_chord]\n",
    "    for ch in decoded_chords[end_chord:]:\n",
    "        if chord2 != ch:\n",
    "            #chord2 = get_diatonic_chord(chord2)\n",
    "            chord2.duration.quarterLength += ch.duration.quarterLength\n",
    "            if isinstance(chord2, chord.Chord):\n",
    "                chord2.closedPosition(forceOctave=3, inPlace=True)\n",
    "\n",
    "    if start_chord != 0:\n",
    "        chord0 = note.Rest()\n",
    "        chord0.duration.quarterLength = 0\n",
    "        for ch in decoded_chords[:start_chord]:\n",
    "            chord0.duration.quarterLength += ch.duration.quarterLength\n",
    "            if isinstance(chord0, chord.Chord):\n",
    "                chord0.closedPosition(forceOctave=3, inPlace=True)\n",
    "        return [chord0, chord1,chord2]\n",
    "\n",
    "    return [chord1,chord2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_chords_rock(decoded_chords):\n",
    "    ''' Reduce total number of chords to only two.\n",
    "    One in the 0 and 3 offset'''\n",
    "    start_chord =0\n",
    "\n",
    "    end_chord = random.randint(start_chord+1,len(decoded_chords)-1)\n",
    "\n",
    "    chord1 = decoded_chords[start_chord]\n",
    "    chord1.duration.quarterLength = 2\n",
    "    if isinstance(chord1, chord.Chord):\n",
    "        chord1.closedPosition(forceOctave=3, inPlace=True)\n",
    "\n",
    "    chord2 = decoded_chords[end_chord]\n",
    "    chord2.duration.quarterLength = 2\n",
    "    if isinstance(chord2, chord.Chord):\n",
    "        chord2.closedPosition(forceOctave=3, inPlace=True)\n",
    "\n",
    "    return [chord1,chord2]\n",
    "    return decoded_chords\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmony(midi_name):\n",
    "    path = midi_name\n",
    "    song, melody_track = open_midi(path,True)\n",
    "    music_key = song.analyze('key')\n",
    "    print(music_key)\n",
    "\n",
    "    ### Encoding melody ###\n",
    "    song_grouped = get_melo_chord(song, melody_track)\n",
    "    input_data = song_grouped['melody'].values\n",
    "\n",
    "    for measure_number in range(len(input_data)):\n",
    "        while len(input_data[measure_number]) < input_max:\n",
    "            input_data[measure_number] = np.append(input_data[measure_number],np.array([zeros]),axis=0)\n",
    "        if len(input_data[measure_number]) > input_max:\n",
    "            input_data[measure_number] = input_data[measure_number][:input_max]\n",
    "    \n",
    "    input_data = np.array(list(input_data))\n",
    "    encoder_song = input_data.reshape((len(input_data),input_max,23))\n",
    "    \n",
    "    ### --------------- ###\n",
    "    \n",
    "    output_melody = song.parts[melody_track]\n",
    "    output_chords = stream.Part()\n",
    "    \n",
    "    ### Decoding harmony ###\n",
    "    \n",
    "    for seq_index in range(len(encoder_song)-1):\n",
    "        input_seq = encoder_song[seq_index: seq_index + 1]\n",
    "        dec_input = [decode_note(x) for x in input_seq[0]]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        \n",
    "        decoded_chords = convert_numeral_chords(dec_input,decoded_sentence,music_key)\n",
    "        decoded_chords = reduce_chords(decoded_chords)\n",
    "\n",
    "        for ch in decoded_chords:\n",
    "            output_chords.append(ch)\n",
    "        decoded_chords = []\n",
    "    \n",
    "    ### ----------------- ###\n",
    "    \n",
    "    return stream.Stream([output_melody,output_chords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a minor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test.midi'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_harmony('melody.mid').write('midi','test.midi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
