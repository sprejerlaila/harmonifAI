{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['including_rests_clean-Copy1.ipynb', '.DS_Store', 'get_music.ipynb', 'all_4.py', 'dict.pkl', 'processed_songs.pkl', '.ipynb_checkpoints', 'run_model.ipynb', 'get_data-Copy1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.options.display.max_rows = 999\n",
    "# Listing current data on our folder.\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "from music21 import converter, corpus, instrument, midi, note, chord, pitch, roman\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Building a english to french translator\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from importlib import reload\n",
    "reload(keras.models)\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "#https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "#https://github.com/devm2024/nmt_keras/blob/master/base.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_songs.pkl', 'rb') as picklefile:\n",
    "    processed_songs = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.concat(processed_songs).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks Devesh Maheshwari for your part in this code!\n",
    "https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['harmony'] = lines['harmony'].str.replace('null','rest')\n",
    "lines['harmony'] = lines['harmony'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary of chords\n",
    "all_chords=set()\n",
    "for harmony in lines.harmony:\n",
    "    for ch in harmony.split():\n",
    "        if ch not in all_chords:\n",
    "            all_chords.add(ch)\n",
    "            \n",
    "target_chords = sorted(list(all_chords))\n",
    "num_decoder_tokens = len(all_chords)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(target_chords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of chords in a measure.\n",
    "lenght_list=[]\n",
    "for l in lines.harmony:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "    \n",
    "# try this length_list = [len(x.split(' ')) for x in lines.harmony]\n",
    "target_max = np.max(lenght_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = np.zeros(\n",
    "    (len(lines.harmony), target_max),\n",
    "    dtype='float32')\n",
    "# Shape: total number of measures, chords per measure.\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(lines.harmony), target_max, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "# Shape: total number of measures, chords per measure, unique chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, target_chords in enumerate(lines.harmony):\n",
    "    for t, ch in enumerate(target_chords.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[ch]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep target_token_index[ch]\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1,target_token_index[ch]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding input data\n",
    "input_max = 16 # Max number of notes per measure\n",
    "# input_max = [len(x) for x in lines.harmony]\n",
    "# input_max = np.max(input_max)\n",
    "num_encoder_tokens = len(lines['melody'].values[0][0]) # 23 = number of unique notes + rest.\n",
    "\n",
    "zeros = [0]*num_encoder_tokens\n",
    "input_data = lines['melody'].values\n",
    "\n",
    "# Unifying the shapes of the measures to create a 3d array. \n",
    "for measure_number in range(len(input_data)):\n",
    "    while len(input_data[measure_number]) < input_max:\n",
    "        input_data[measure_number] = np.append(input_data[measure_number],np.array([zeros]),axis=0)\n",
    "    if len(input_data[measure_number]) > input_max:\n",
    "        input_data[measure_number] = input_data[measure_number][:input_max]\n",
    "\n",
    "\n",
    "input_data = np.array(list(input_data))\n",
    "encoder_input_data = input_data.reshape((len(input_data),input_max,num_encoder_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input sequence\n",
    "encoder_inputs = Input(shape=(None,num_encoder_tokens))\n",
    "encoder = LSTM(50, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state. (None, num_decoder_tokens?)\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dex = Embedding(num_decoder_tokens, embedding_size) # num_decoder_tokens = number of unique chords.\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn encoder inputs and decoder inputs into decoder outputs.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer; adam rmsprop\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None, 23)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     5200        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50), (None,  14800       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 50), ( 20200       embedding_1[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 104)    5304        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 45,504\n",
      "Trainable params: 45,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),ModelCheckpoint(filepath='best_model_farm.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 257 samples, validate on 65 samples\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 0s 833us/step - loss: 1.3791 - acc: 0.0707 - val_loss: 2.5347 - val_acc: 0.0615\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 0s 693us/step - loss: 1.3814 - acc: 0.0683 - val_loss: 2.5537 - val_acc: 0.0598\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 0s 725us/step - loss: 1.3734 - acc: 0.0750 - val_loss: 2.5874 - val_acc: 0.0581\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 0s 626us/step - loss: 1.3780 - acc: 0.0737 - val_loss: 2.5915 - val_acc: 0.0607\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 0s 692us/step - loss: 1.3757 - acc: 0.0741 - val_loss: 2.5478 - val_acc: 0.0607\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 0s 616us/step - loss: 1.3629 - acc: 0.0770 - val_loss: 2.5050 - val_acc: 0.0581\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 0s 588us/step - loss: 1.3645 - acc: 0.0778 - val_loss: 2.5112 - val_acc: 0.0607\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 0s 621us/step - loss: 1.3560 - acc: 0.0774 - val_loss: 2.5321 - val_acc: 0.0607\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 0s 625us/step - loss: 1.3548 - acc: 0.0774 - val_loss: 2.4938 - val_acc: 0.0607\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 0s 598us/step - loss: 1.3521 - acc: 0.0776 - val_loss: 2.4783 - val_acc: 0.0607\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 0s 655us/step - loss: 1.3502 - acc: 0.0770 - val_loss: 2.4903 - val_acc: 0.0607\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 0s 715us/step - loss: 1.3480 - acc: 0.0750 - val_loss: 2.4996 - val_acc: 0.0607\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 0s 619us/step - loss: 1.3462 - acc: 0.0748 - val_loss: 2.4719 - val_acc: 0.0607\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 0s 617us/step - loss: 1.3405 - acc: 0.0804 - val_loss: 2.4700 - val_acc: 0.0607\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 0s 599us/step - loss: 1.3364 - acc: 0.0806 - val_loss: 2.5114 - val_acc: 0.0607\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 0s 616us/step - loss: 1.3343 - acc: 0.0798 - val_loss: 2.5175 - val_acc: 0.0607\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 0s 619us/step - loss: 1.3318 - acc: 0.0824 - val_loss: 2.5256 - val_acc: 0.0607\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 0s 630us/step - loss: 1.3313 - acc: 0.0828 - val_loss: 2.5465 - val_acc: 0.0607\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 0s 654us/step - loss: 1.3284 - acc: 0.0834 - val_loss: 2.5444 - val_acc: 0.0641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1d5ddef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size = 512?\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          validation_split=0.20, callbacks = callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# model_json = model.to_json()\n",
    "# with open(\"models/final_test/model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"models/final_test/model.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 23)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 50), (None, 50),  14800     \n",
      "=================================================================\n",
      "Total params: 14,800\n",
      "Trainable params: 14,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2= dex(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'input_4:0' shape=(?, 50) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = encoder_model.to_json()\n",
    "with open(\"encoder_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "encoder_model.save_weights(\"encoder_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = decoder_model.to_json()\n",
    "with open(\"decoder_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "decoder_model.save_weights(\"decoder_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "\n",
    "with open('target_token_index.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(target_token_index, picklefile)\n",
    "\n",
    "\n",
    "with open('reverse_target_char_index.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(reverse_target_char_index ,picklefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
